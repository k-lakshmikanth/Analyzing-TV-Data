{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPNk2ndZ7d3hSErX2nElga3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0CA1LaRWxGyN"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gpt4all"
      ],
      "metadata": {
        "id": "Ca0UDCKn5xEH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "# from langchain.llms import GPT4All\n",
        "from gpt4all import GPT4All\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler"
      ],
      "metadata": {
        "id": "i8_HcWc5xg52"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "EqvQEGs1ziCN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"ggml-model-gpt4all-falcon-q4_0\""
      ],
      "metadata": {
        "id": "wCXndGglznXO"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks support token-wise streaming\n",
        "callbacks = [StreamingStdOutCallbackHandler()]\n",
        "\n",
        "# Verbose is required to pass to the callback manager\n",
        "llm = GPT4All(model_name,\\\n",
        "              allow_download=True,\\\n",
        "            #   callbacks=callbacks,\\\n",
        "            #   verbose=True\\\n",
        "              )\n",
        "\n",
        "# If you want to use a custom model add the backend parameter\n",
        "# Check https://docs.gpt4all.io/gpt4all_python.html for supported backends\n",
        "# llm = GPT4All(model=local_path, backend=\"gptj\", callbacks=callbacks, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSCgcn5o2eBL",
        "outputId": "da822d1d-ab1c-44f7-c47c-a0d178d31256"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.06G/4.06G [00:44<00:00, 92.1MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model downloaded at:  /root/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = \"/root/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
        "\n",
        "from langchain.llms import GPT4All\n",
        "\n",
        "llm = GPT4All(model=local_path, callbacks=callbacks, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NH63CmW-_dqd",
        "outputId": "e511985b-4e79-437e-a4de-92b0d10232ce"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found model file at  /root/.cache/gpt4all/ggml-model-gpt4all-falcon-q4_0.bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "oGVecpH7BbdC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What NFL team won the Super Bowl in the year Justin Bieber was born?\"\n",
        "\n",
        "llm_chain.run(question)"
      ],
      "metadata": {
        "id": "21dGuj0nBcUN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}